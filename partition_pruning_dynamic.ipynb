{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccd32463-7ae3-4748-9a4b-55d037f45b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Partition pruning \n",
    "- a Spark optimization where Spark reduces the amount of data read by skipping partitions that don't match the filter condition.\n",
    "\n",
    "Static Partition Pruning:\n",
    "- When partitions can be evaluated statically during query planning (before execution), Spark directly prunes irrelevant partitions in the query plan.\n",
    "Photon supports operations involving statically pruned partitions.\n",
    "\n",
    "No Partition Pruning:\n",
    "- If no partitions are available or if the query involves a non-partitioned DataFrame, Photon cannot perform the optimizations tied to partitioning and pruning. As a result, Photon may fall back to default Spark query planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d29aed7-94fa-4504-9ae2-ad93057f833f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('PartitionPurning').getOrCreate()\n",
    "\n",
    "# Static Partition Pruning\n",
    "data = [(1, \"A\"), (2, \"B\"), (3, \"A\"), (4, \"C\"), (5, \"A\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\"]).repartition(\"value\")\n",
    "\n",
    "filter_condition = \"value = 'A'\"\n",
    "result_with_partition_pruning = df.filter(filter_condition)\n",
    "\n",
    "print('result with Patition Purning:')\n",
    "result_with_partition_pruning.show()\n",
    "print('Execution Plan with Partition Purning:')\n",
    "result_with_partition_pruning.explain()\n",
    "\n",
    "# No Partition Pruning\n",
    "data = [(1, \"A\"), (2, \"B\"), (3, \"A\"), (4, \"C\"), (5, \"A\")]\n",
    "df2 = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "\n",
    "filter_condition = \"value = 'A'\"\n",
    "\n",
    "result_without_partition_pruning = df2.filter(filter_condition)\n",
    "\n",
    "print('result plan without partition purning:')\n",
    "result_without_partition_pruning.show()\n",
    "print('execution plan without partition purning:')\n",
    "result_without_partition_pruning.explain()\n",
    "\n",
    "# Output: == Photon Explanation ==\n",
    "# Photon does not fully support the query because:\n",
    "# \t\tUnsupported node: LocalTableScan [id#11575L, value#11576].\n",
    "\n",
    "# Photon does not fully support the query without partition pruning because:\n",
    "# The DataFrame is not partitioned, so the query requires a LocalTableScan that operates entirely on the driver node.\n",
    "# Photon optimizations are designed for partitioned data processing or distributed execution, which are absent when LocalTableScan is used.\n",
    "# Photon falls back to default Spark behavior when it cannot efficiently process LocalTableScan nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7114592d-e280-48f0-99ac-d6ed21ace307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9614c9-1af4-4854-aed8-990e11598dcb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dynamic"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PartitionOverwriteExample\")\\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data about customers from different months\n",
    "data = [(\"Rajesh\", \"2023\", \"07\"), (\"Rajesh\", \"2023\", \"08\"), (\"Sunita\", \"2023\", \"09\")]\n",
    "columns = [\"name\", \"year\", \"month\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Write data partitioned by year and month\n",
    "df.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").format(\"parquet\").save(\"/Volumes/workspace/default/spark_vol1/partition_dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcea3ae0-c123-480e-94f2-d34e72f5a822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_jul = spark.read.format(\"parquet\").load(\"/Volumes/workspace/default/spark_vol1/partition_dynamic/year=2023/month=07\")\n",
    "df_aug = spark.read.format(\"parquet\").load(\"/Volumes/workspace/default/spark_vol1/partition_dynamic/year=2023/month=08\")\n",
    "def_sep = spark.read.format(\"parquet\").load(\"/Volumes/workspace/default/spark_vol1/partition_dynamic/year=2023/month=09\")\n",
    "df_jul.show()\n",
    "df_aug.show()\n",
    "def_sep.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cddb5be3-0993-4e8e-82d8-8577b6507b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fbf6b95-99d6-4509-a43e-8912fa41b04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"Update\", \"2023\", \"07\")]\n",
    "columns = [\"name\", \"year\", \"month\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "df.write.option(\"partitionOverwriteMode\", \"dynamic\").partitionBy(\"year\", \"month\").mode(\"overwrite\").format(\"parquet\").save(\"/Volumes/workspace/default/spark_vol1/partition_dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da15f7c1-758e-40d7-85dd-ccdd91a6ab1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_jul = spark.read.format(\"parquet\").load(\"/Volumes/workspace/default/spark_vol1/partition_dynamic/year=2023/month=07\")\n",
    "df_aug = spark.read.format(\"parquet\").schema(schema).load(\"/Volumes/workspace/default/spark_vol1/partition_dynamic/year=2023/month=08\")\n",
    "def_sep = spark.read.format(\"parquet\").schema(schema).load(\"/Volumes/workspace/default/spark_vol1/partition_dynamic/year=2023/month=09\")\n",
    "df_jul.show()\n",
    "df_aug.show()\n",
    "def_sep.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbefe0f8-c772-4e51-8afe-8b4f0478969d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "partition_pruning_dynamic",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
