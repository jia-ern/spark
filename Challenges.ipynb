{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ae2c91-68a8-4703-a115-28fb76d3cd6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert index of a PySpark DataFrame into a column"
    }
   },
   "outputs": [],
   "source": [
    "# How to convert the index of a PySpark DataFrame into a column?\n",
    "# +-------+-----+\n",
    "# | Name|Value|\n",
    "# +-------+-----+\n",
    "# | Alice| 1|\n",
    "# | Bob| 2|\n",
    "# |Charlie| 3|\n",
    "# +-------+-----+\n",
    "\n",
    "# Expected output:\n",
    "# +-------+-----+-----+\n",
    "# | Name|Value|index|\n",
    "# +-------+-----+-----+\n",
    "# | Alice| 1| 0|\n",
    "# | Bob| 2| 1|\n",
    "# |Charlie| 3| 2|\n",
    "# +-------+-----+-----+\n",
    "\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "print(w)\n",
    "\n",
    "# add index column\n",
    "# row_number() function is designed to start numbering from 1, and not from 0\n",
    "# hence need to add -1\n",
    "df_window_monotonically = df.withColumn(\"index\", row_number().over(w) - 1)\n",
    "df_window_monotonically.show()\n",
    "\n",
    "df_monotonically = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "df_monotonically.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0047e3bc-d12f-4f72-ba81-8b634366295e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Combine many lists to form a PySpark DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "# How to combine many lists to form a PySpark DataFrame\n",
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]\n",
    "\n",
    "data = zip(list1,list2)\n",
    "df = spark.createDataFrame(data, [\"col_list1\", \"col_list2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617b797f-39b5-4ca8-b12b-f85b7ce2d921",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the items of list A not present in list B"
    }
   },
   "outputs": [],
   "source": [
    "# How to get the items of list A not present in list B\n",
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "\n",
    "# in method\n",
    "filtered_list = [i for i in list_A if i not in list_B]\n",
    "print(filtered_list)\n",
    "\n",
    "# execptAll method with df\n",
    "df_A = spark.createDataFrame(list_A, [\"value\"])\n",
    "df_B = spark.createDataFrame(list_B, [\"value\"])\n",
    "\n",
    "result_df = df_A.exceptAll(df_B)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fe02dc-a151-4a09-b4cb-a21236a10988",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the items not common to both list A and list B"
    }
   },
   "outputs": [],
   "source": [
    "# How to get the items not common to both list A and list B\n",
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "\n",
    "# Approach 1: Using List Comprehension - worst performance\n",
    "filtered_list_A = [i for i in list_A if i not in list_B]\n",
    "filtered_list_B = [i for i in list_B if i not in list_A]\n",
    "filtered_list = filtered_list_A + filtered_list_B\n",
    "print(list(set(filtered_list)))\n",
    "\n",
    "# Approach 2: Using Sets for Symmetric Difference - best approach\n",
    "filtered_list = set(list_A).symmetric_difference(list_B)\n",
    "print(list(filtered_list))\n",
    "\n",
    "# Approach 3: Using a Set Union Minus an Intersection\n",
    "set_A_B = set(list_A) | set(list_B)\n",
    "intersect_A_B = set(list_A) & set(list_B)\n",
    "filtered_list = set_A_B - intersect_A_B\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4330f3ba-b94b-4cb8-ac21-a9d2b221baed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "min, 25th percentile, median, 75th, max"
    }
   },
   "outputs": [],
   "source": [
    "# How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?\n",
    "\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Approach 1: Using approxQuantile()\n",
    "# O(n log(n))\n",
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "print(quantiles)\n",
    "print(\"Min: \", quantiles[0])\n",
    "print(\"25th percentile: \", quantiles[1])\n",
    "print(\"Median: \", quantiles[2])\n",
    "print(\"75th percentile: \", quantiles[3])\n",
    "print(\"Max: \", quantiles[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cda0c7b-6c23-45cf-a5aa-dfa34bdebef7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get frequency counts of unique items of a column"
    }
   },
   "outputs": [],
   "source": [
    "# How to get frequency counts of unique items of a column?\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "distinct_job = df.groupBy(\"job\").count()\n",
    "distinct_job.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520e3d24-c7a6-4a47-96c6-ff9c864503e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "df_distinct = df.groupBy(\"job\").count()\n",
    "df_distinct = df_distinct.orderBy(\"count\", ascending=False).limit(2).select(\"job\")\n",
    "distinct_job_list = [row['job'] for row in df_distinct.collect()]\n",
    "print(distinct_job_list)\n",
    "\n",
    "df_with_replacement = df.withColumn(\"job\", F.when(F.col(\"job\").isin(distinct_job_list), F.col(\"job\")).otherwise(\"Other\"))\n",
    "df_with_replacement.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1255626a-abd8-4efa-8517-8eff4c9cadfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get frequency counts of unique items of a column for the top 2 and combine the remainng as others\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "Row(name='Sam', job='Teacher'),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "df_distinct = df.groupBy(\"job\").count().limit(2).select(\"job\")\n",
    "distinct_job_list = [row[\"job\"] for row in df_distinct.collect()]\n",
    "print(distinct_job_list)\n",
    "\n",
    "df_distinct_count = df.withColumn(\"job\", F.when(F.col(\"job\").isin(distinct_job_list), F.col(\"job\")).otherwise(\"Other\"))\n",
    "df_distinct_count = df_distinct_count.groupBy(\"job\").count()\n",
    "df_distinct_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b556d3-2da4-489c-9344-bfd57d243c03",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop rows with NA values specific to a particular column"
    }
   },
   "outputs": [],
   "source": [
    "# How to Drop rows with NA values specific to a particular column\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.dropna(subset=[\"Value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b528b1f-6682-4f52-ac62-6c8922bf371f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "rename columns of a PySpark DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "# How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names\n",
    "\n",
    "# suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "df.show()\n",
    "\n",
    "for i in range(len(old_names)):\n",
    "    df = df.withColumnRenamed(old_names[i], new_names[i])\n",
    "df.show()\n",
    "\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00591a2-b247-480a-918d-1227d08bbf79",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bin a numeric list to 10 groups of equal size"
    }
   },
   "outputs": [],
   "source": [
    "# How to bin a numeric list to 10 groups of equal size\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
    "num_items = 100\n",
    "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "num_buckets = 10\n",
    "quantiles = df.stat.approxQuantile(\"values\", [i/num_buckets for i in range(num_buckets+1)], 0.01)\n",
    "print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d455a3-8bd6-4d28-ad9b-0826729b3e30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create contigency table?"
    }
   },
   "outputs": [],
   "source": [
    "# How to create contigency table?\n",
    "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
    "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
    "df.show()\n",
    "\n",
    "# Creating a cube on ‘category1’, and calculate the number of rows in each dimensional.\n",
    "# Frequency\n",
    "df.cube(\"category1\").count().show()\n",
    "\n",
    "# Contingency table\n",
    "df.crosstab('category1', 'category2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf9220e-5f1d-4c92-a8c8-a54edd16cafc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Find the numbers that are multiples of 3 from a column?"
    }
   },
   "outputs": [],
   "source": [
    "# How to find the numbers that are multiples of 3 from a column?\n",
    "\n",
    "from pyspark.sql.functions import rand, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"is_multiple_of_3\", F.when(F.col(\"random\") % 3 == 0, 1).otherwise(0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c6bb62-0420-4dd8-9ff4-4a6b39634c7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract items at given positions from a column?"
    }
   },
   "outputs": [],
   "source": [
    "# How to extract items at given positions from a column?\n",
    "\n",
    "from pyspark.sql.functions import rand, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "# Generates random numbers between 0.0 and 1.0 for each row in the DataFrame.\n",
    "# The seed ensures the sequence of random numbers is reproducible.\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "pos = [0, 4, 8, 5]\n",
    "\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "df = df.withColumn(\"index\", row_number().over(window) - 1)\n",
    "df.show()\n",
    "df = df.filter(F.col(\"index\").isin(pos))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063480b4-7c35-4fa1-b1ef-06fb294cf195",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stack 2 DataFrames vertically"
    }
   },
   "outputs": [],
   "source": [
    "# How to stack two DataFrames vertically ?\n",
    "\n",
    "# Create DataFrame for region A\n",
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()\n",
    "\n",
    "# Create DataFrame for region B\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_B.show()\n",
    "\n",
    "df = df_A.unionAll(df_B)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ad0428-5033-4a86-9d8d-bc083c76dfc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute mean squared error on a truth & predicted columns"
    }
   },
   "outputs": [],
   "source": [
    "# How to compute the mean squared error on a truth and predicted columns\n",
    "\n",
    "data = [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
    "df = spark.createDataFrame(data, [\"actual\", \"predicted\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"squared_error\", pow((col(\"actual\") - col(\"predicted\")), 2))\n",
    "df.show()\n",
    "\n",
    "mse = df.agg({\"squared_error\":\"avg\"}).collect()[0][0]\n",
    "print(f\"Mean Squared Error (MSE) = {mse}\")\n",
    "\n",
    "mse = df.select(F.avg(F.col(\"squared_error\"))).collect()[0][0]\n",
    "print(f\"Mean Squared Error (MSE) = {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d705bf-9125-42e9-a115-9da0f71ded54",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute difference of differences between consecutive no. of a column"
    }
   },
   "outputs": [],
   "source": [
    "# How to compute difference of differences between consecutive numbers of a column\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "df = df.withColumn(\"id\", row_number().over(window) - 1)\n",
    "\n",
    "df = df.withColumn(\"prev_salary\", F.lag(F.col(\"salary\")).over(window))\n",
    "\n",
    "df = df.withColumn(\"salary_difference\", F.when(\n",
    "    F.col(\"prev_salary\").isNotNull(), \n",
    "    F.col(\"salary\")-F.col(\"prev_salary\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447dc259-372c-4346-8d5d-1e1e1ca2b5e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the day of month, week number, day of year and day of week from a date strings"
    }
   },
   "outputs": [],
   "source": [
    "# How to get the day of month, week number, day of year and day of week from a date strings\n",
    "\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# if the date string format is correct, then can use date casting\n",
    "df = df.withColumn(\"date_1\", F.col(\"date_str_1\").cast(\"date\"))\n",
    "\n",
    "# if the date is malformed, then use to_date\n",
    "df = df.withColumn(\"date_2\", F.to_date(F.col(\"date_str_2\"), 'dd MMM yyyy')) \\\n",
    "    .withColumn(\"day_of_month\", F.dayofmonth(F.col(\"date_1\"))) \\\n",
    "    .withColumn(\"week_of_year\", F.weekofyear(F.col(\"date_1\"))) \\\n",
    "    .withColumn(\"day_of_year\", F.dayofyear(F.col(\"date_1\"))) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(F.col(\"date_1\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c97e2c-6126-4822-b2e4-294d892b696e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Date"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "HUMAN_READABLE_TIMESTAMP_FORMAT = \"%Y%m%d_%H%M%S\"\n",
    "\n",
    "def human_readable_to_timestamp(yyyyMMdd_HHmmss: str) -> datetime.datetime:\n",
    "    return datetime.datetime.strptime(\n",
    "        yyyyMMdd_HHmmss, HUMAN_READABLE_TIMESTAMP_FORMAT\n",
    "    ).replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(human_readable_to_timestamp('20230915_123456'))\n",
    "print(human_readable_to_timestamp('20240522_080000'))\n",
    "\n",
    "timestamp_format = \"yyyyMMdd_HHmmss\"\n",
    "data = [(\"20230915_123456\",), (\"20240522_080000\",)]\n",
    "df = spark.createDataFrame(data, [\"human_readable_timestamp\"])\n",
    "df = df.withColumn(\"datetime_utc\", F.to_timestamp(\"human_readable_timestamp\", timestamp_format))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a96aa991-97e9-4527-9d9f-e7cd873eecb6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert year-month string to dates corresponding to the 4th day of the month"
    }
   },
   "outputs": [],
   "source": [
    "# How to convert year-month string to dates corresponding to the 4th day of the month\n",
    "\n",
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"converted_date\", F.to_date(F.col(\"MonthYear\"), 'MMM yyyy'))\n",
    "df = df.withColumn(\"final_date\", F.date_add(F.col(\"converted_date\"), 3))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbeb7799-14d2-4ce8-a0e1-15e652f22e6c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter words that contain atleast 2 vowels from a series"
    }
   },
   "outputs": [],
   "source": [
    "# How to filter words that contain atleast 2 vowels from a series?\n",
    "\n",
    "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
    "df.show()\n",
    "\n",
    "vowels = 'AEIOUaeiou'\n",
    "df_translated = df.withColumn(\"translated\", F.translate(F.col('Word'), 'AEIOUaeiou', ''))\n",
    "df_translated.show()\n",
    "\n",
    "\n",
    "# Method 1: using translate\n",
    "df_filtered = df.where((F.length(col('Word')) - F.length(F.translate(col('Word'), 'AEIOUaeiou', ''))) >= 2)\n",
    "# or\n",
    "df_filtered = df.filter((F.length(col('Word')) - F.length(F.translate(col('Word'), 'AEIOUaeiou', ''))) >= 2)\n",
    "df_filtered.show()\n",
    "\n",
    "\n",
    "# Method 2: using expr or regexp_extract\n",
    "df_filtered = df.filter(\n",
    "    F.length(F.regexp_replace(F.col(\"Word\"), \"[^AEIOUaeiou]\", \"\")) >= 2\n",
    ")\n",
    "df_filtered.show()\n",
    "\n",
    "df_filtered = df.filter(\n",
    "    F.expr(\"length(regexp_replace(Word, '[^AEIOUaeiou]', '')) >= 2\")\n",
    ")\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "240eab10-a267-4659-aca8-66c7272ed099",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter valid emails from a list"
    }
   },
   "outputs": [],
   "source": [
    "# How to filter valid emails from a list?\n",
    "\n",
    "import re\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "\n",
    "def check(email):\n",
    "\n",
    "    # pass the regular expression\n",
    "    # and the string into the fullmatch() method\n",
    "    if(re.fullmatch(regex, email)):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "check(\"ankitrai326@gmail.com\")\n",
    "check(\"my.ownsite@ourearth.org\")\n",
    "check(\"ankitrai326.com\")\n",
    "\n",
    "check_mail = udf(check, BooleanType())\n",
    "\n",
    "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
    "\n",
    "# Convert the list to DataFrame\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "df.show(truncate =False)\n",
    "\n",
    "df = df.withColumn(\"is_valid_email\", check_mail(F.col(\"value\")))\n",
    "df.show()\n",
    "\n",
    "df = df.filter(check_mail(F.col(\"value\"))).select(\"Value\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f6c657-f702-40ce-a9c6-abd4f01f2d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pivot PySpark DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "# How to Pivot PySpark DataFrame?\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f54b8c-0010-4461-92e5-e5d3e88c4977",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the mean of a variable grouped by another variable"
    }
   },
   "outputs": [],
   "source": [
    "# How to get the mean of a variable grouped by another variable?\n",
    "\n",
    "# Sample data\n",
    "data = [(\"1001\", \"Laptop\", 1000),\n",
    "(\"1002\", \"Mouse\", 50),\n",
    "(\"1003\", \"Laptop\", 1200),\n",
    "(\"1004\", \"Mouse\", 30),\n",
    "(\"1005\", \"Smartphone\", 700)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.groupBy(\"Product\").agg(F.mean(F.col(\"Price\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be05101-2396-432b-bc57-b73a26481041",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Replace missing spaces in a string with the least frequent character"
    }
   },
   "outputs": [],
   "source": [
    "# How to replace missing spaces in a string with the least frequent character?\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "df = spark.createDataFrame([('dbcc debabedgade',),], [\"string\"])\n",
    "df.show()\n",
    "\n",
    "def check_least_frequent_char(string_value):\n",
    "    counter = Counter(string_value.replace(' ', ''))\n",
    "    least_frequent_character = min(counter, key=counter.get)\n",
    "    return string_value.replace(\" \", least_frequent_character)\n",
    "\n",
    "udf_check_least_frequent_char = udf(check_least_frequent_char, StringType())\n",
    "\n",
    "df = df.withColumn(\"modified_string\", udf_check_least_frequent_char(F.col(\"string\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d4f2d9-7c55-41fd-9655-c8bca4fc0c3e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Array"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, array_sort, array_union, array_intersect\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"array_functions_example\").getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "data = [(\"Alice\", [2, 4, 6]), \n",
    "        (\"Bob\", [1, 2, 3]),\n",
    "        (\"Charlie\", [4, 5, 6])]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Numbers\"])\n",
    "df.show(truncate=False)\n",
    "\n",
    "# array_contains\n",
    "df_after = df.filter(array_contains(df.Numbers, 4))\n",
    "df_after.show()\n",
    "\n",
    "# array_contains\n",
    "df_after = df.select(\n",
    "        \"Name\", \n",
    "        array_sort(F.col(\"Numbers\")).alias(\"sorted_numbers\")\n",
    "    )\n",
    "df_after.show()\n",
    "\n",
    "df.select(\n",
    "        \"Name\",\n",
    "        array_union(F.col(\"Numbers\"), F.array(F.lit(9))).alias(\"union_numbers\")\n",
    "    ).show(truncate=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3abe2cfa-d0e3-4d39-821b-1193c84373cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a TimeSeries starting ‘2000-01-01’ & 10 weekends (saturdays) after that having random numbers as values?"
    }
   },
   "outputs": [],
   "source": [
    "# How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?\n",
    "\n",
    "from pyspark.sql.functions import expr, explode, sequence, rand\n",
    "\n",
    "# Start date and end date (start + 10 weekends)\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2000-03-04' # Calculated manually: 10 weekends (Saturdays) from start date\n",
    "\n",
    "df = spark.range(1).select(\n",
    "    explode(\n",
    "        sequence(\n",
    "            expr(f\"date'{start_date}'\"),\n",
    "            expr(f\"date('{end_date}')\"),\n",
    "            expr(\"interval 1 day\")\n",
    "        )\n",
    "    ).alias(\"date\")\n",
    ")\n",
    "\n",
    "df = df.filter(F.dayofweek(F.col(\"date\"))==7)\n",
    "df = df.withColumn(\"random_num\", ((rand(seed=42)*10) +1).cast(\"int\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65dbb65c-99cb-4383-9273-903c0ebaf64c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import Column, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def integration_succeed(http_status: Column) -> Column:\n",
    "    return http_status.cast(IntegerType()).between(F.lit(200), F.lit(300))\n",
    "\n",
    "def integration_succeed(http_status: Column) -> Column:\n",
    "    return http_status.cast(IntegerType()).between(F.lit(200), F.lit(300))  # Success range: [200, 300)\n",
    "\n",
    "# Sample DataFrame with HTTP status codes\n",
    "test_data = [\n",
    "    (\"200\",),   # Success case\n",
    "    (\"201\",),   # Success case\n",
    "    (\"299\",),   # Success case\n",
    "    (\"300\",),   # Out of range\n",
    "    (\"404\",),   # Out of range (failure)\n",
    "    (\"500\",),   # Out of range (failure)\n",
    "    (None,),    # Missing value\n",
    "]\n",
    "\n",
    "# Define schema and create the DataFrame\n",
    "df = spark.createDataFrame(test_data, [\"http_status\"])\n",
    "\n",
    "# Apply the `integration_succeed` function and create a new column to test it\n",
    "df = df.withColumn(\"integration_success\", integration_succeed(col(\"http_status\")))\n",
    "\n",
    "# Show the results\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69c94cb4-3cdb-48ba-9796-a0a71489385d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "count of missing values in each column"
    }
   },
   "outputs": [],
   "source": [
    "# How to check if a dataframe has any missing values and count of missing values in each column?\n",
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "missing = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n",
    "has_missing = any(row.asDict().values() for row in missing.collect())\n",
    "print(has_missing)\n",
    "\n",
    "missing_count = missing.collect()[0].asDict()\n",
    "print(missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e08d269-edf5-4e6b-b4f7-6cb88a655085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "missing = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "missing.show()\n",
    "\n",
    "# dict in string\n",
    "result = [row.asDict() for row in missing.collect()]\n",
    "print(result)\n",
    "\n",
    "# dict\n",
    "missing_count = df.select([(F.count(F.when(F.col(c).isNull(), c)).alias(c)) for c in df.columns]).collect()[0].asDict()\n",
    "print(missing_count)\n",
    "\n",
    "has_missing = any(v > 0 for v in missing_count.values())\n",
    "print(has_missing)\n",
    "\n",
    "# dict\n",
    "column_expressions=[]\n",
    "for c in df.columns:\n",
    "    # Create the expression to count null values for each column\n",
    "    column_expression = sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    # Append the expression to the list\n",
    "    column_expressions.append(column_expression)\n",
    "\n",
    "print(\"column_expressions\", column_expressions)\n",
    "# Use the column expressions in the select statement\n",
    "missing = df.select(column_expressions)\n",
    "\n",
    "print(\"wthout *\")\n",
    "missing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3844cfd4-5c4d-4904-9e76-e15b7ecd7aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame([\n",
    "    (\"A\", 1, None),\n",
    "    (\"B\", None, \"123\"),\n",
    "    (\"C\", 3, None)\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "# Prepare list of column expressions to count nulls\n",
    "column_expressions = [\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)  # Count nulls for each column\n",
    "    for c in df.columns\n",
    "]\n",
    "\n",
    "# Correct: Use * to unpack the list\n",
    "missing = df.select(column_expressions)\n",
    "missing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3e25800-336c-49a3-9836-75c079a8cdfb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Replace missing values of multiple numeric columns with the mean"
    }
   },
   "outputs": [],
   "source": [
    "# How to replace missing values of multiple numeric columns with the mean\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, 123 ),\n",
    "(\"B\", 3, 456),\n",
    "(\"D\", 6, None),\n",
    "], [\"Name\", \"var1\", \"var2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "fill_na_list = [\"var1\", \"var2\"]\n",
    "\n",
    "column_expressions = []\n",
    "\n",
    "for c in fill_na_list:\n",
    "    column_expression = F.mean(F.col(c)).alias(c)\n",
    "    column_expressions.append(column_expression)\n",
    "\n",
    "mean_values_row = df.select(*column_expressions).collect()[0]\n",
    "mean_values = {col: mean_values_row[col] for col in fill_na_list}\n",
    "print(mean_values)\n",
    "\n",
    "mean_values_row = df.select(\n",
    "    *[\n",
    "        F.mean(F.col(c)).alias(c)\n",
    "        for c in fill_na_list\n",
    "    ]\n",
    ").collect()[0]\n",
    "print(mean_values_row)\n",
    "\n",
    "mean_values = {col: mean_values_row[col] for col in fill_na_list}\n",
    "print(mean_values)\n",
    "\n",
    "df_filled = df.fillna(mean_values)\n",
    "\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce509dd1-df9e-432d-b675-69a727acf028",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Format or suppress scientific notations in a PySpark DataFrame?"
    }
   },
   "outputs": [],
   "source": [
    "# How to format or suppress scientific notations in a PySpark DataFrame?\n",
    "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "decimal_point = 5\n",
    "\n",
    "df = df.withColumn(\"formatted_number\", format_number(F.col(\"your_column\"), decimal_point))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4b77e4-b61d-45c2-a6d0-534e0250f177",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Format all the values in a dataframe as percentages?"
    }
   },
   "outputs": [],
   "source": [
    "# How to format all the values in a dataframe as percentages?\n",
    "\n",
    "data = [(0.1, .08), (0.2, .06), (0.33, .02)]\n",
    "df = spark.createDataFrame(data, [\"numbers_1\", \"numbers_2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"numbers_1\", F.concat((F.col(\"numbers_1\")*100).cast(\"string\"), F.lit('%'))) \\\n",
    "        .withColumn(\"numbers_2\", F.concat((F.col(\"numbers_2\")*100).cast(\"string\"), F.lit('%')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a76189f-efa0-4234-b012-920a385b45dd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter every nth row in a dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# How to filter every nth row in a dataframe\n",
    "\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3), (\"Dave\", 4), (\"Eve\", 5),\n",
    "(\"Frank\", 6), (\"Grace\", 7), (\"Hannah\", 8), (\"Igor\", 9), (\"Jack\", 10)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Number\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "nth_row = 5\n",
    "\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "df = df.withColumn(\"index\", row_number().over(window))\n",
    "\n",
    "df = df.filter(df.index % nth_row == 0)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992d2ca6-2862-4ca7-bdd8-2664a86e19d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculate missing value percentage in each column?"
    }
   },
   "outputs": [],
   "source": [
    "# How to calculate missing value percentage in each column?\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = [(\"John\", \"Doe\", None),\n",
    "(None, \"Smith\", \"New York\"),\n",
    "(\"Mike\", \"Smith\", None),\n",
    "(\"Anna\", \"Smith\", \"Boston\"),\n",
    "(None, None, None)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"FirstName\", \"LastName\", \"City\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "total_row = df.count()\n",
    "\n",
    "for c in df.columns:\n",
    "    null_value = df.filter(F.col(c).isNull()).count()\n",
    "    print(\"Missing values in column {c}: \", null_value/total_row * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e0670f-00e1-4f67-8cc9-5db263182bf3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the last n rows of a dataframe with row sum > 100"
    }
   },
   "outputs": [],
   "source": [
    "# How to get the last n rows of a dataframe with row sum > 100\n",
    "\n",
    "# Sample data\n",
    "data = [(10, 25, 70),\n",
    "(40, 5, 20),\n",
    "(70, 80, 100),\n",
    "(10, 2, 60),\n",
    "(40, 50, 20)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Display original DataFrame\n",
    "df.show()\n",
    "\n",
    "from functools import reduce\n",
    "df = df.withColumn(\"accumulative_sum\", reduce(lambda x, y: x + y, [F.col(c) for c in df.columns]))\n",
    "df.show()\n",
    "\n",
    "df = df.filter(df.accumulative_sum > 100)\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id()).sort(\"id\", ascending=False).limit(2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8bb28fb-1d1a-4e82-a70a-7b2acc88da5b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a column that contains the penultimate value in each row?"
    }
   },
   "outputs": [],
   "source": [
    "# How to create a column that contains the penultimate value in each row?\n",
    "\n",
    "data = [(10, 20, 30),\n",
    "(40, 60, 50),\n",
    "(80, 70, 90)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "# Define UDF to sort array in descending order\n",
    "sort_array_desc = F.udf(lambda arr: sorted(arr), ArrayType(IntegerType()))\n",
    "\n",
    "# Create array from columns, sort in descending order and get the penultimate value\n",
    "df = df.withColumn(\"row_as_array\", sort_array_desc(F.array(df.columns)))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"Penultimate\", df['row_as_array'].getItem(1))\n",
    "df.show()\n",
    "\n",
    "df = df.drop('row_as_array')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0603a55d-7c6c-4aa8-be62-89cc832b8765",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Method 2"
    }
   },
   "outputs": [],
   "source": [
    "# How to create a column that contains the penultimate value in each row?\n",
    "\n",
    "data = [(10, 20, 30),\n",
    "(40, 60, 50),\n",
    "(80, 70, 90)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"row_array\", F.array(df.columns))\n",
    "df.show()\n",
    "df =  df.withColumn(\"reversed_row_array\", F.sort_array(F.col(\"row_array\"), asc=False))\n",
    "df.show()\n",
    "df = df.withColumn(\"penultimate\", F.col(\"reversed_row_array\")[1])\n",
    "df.select(\"Column1\", \"Column2\", \"Column3\", \"penultimate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0eaf85-06c0-43b1-aed5-1159fe1e38a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create lags and leads of a column by group in a dataframe?"
    }
   },
   "outputs": [],
   "source": [
    "# How to create lags and leads of a column by group in a dataframe?\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"2023-01-01\", \"Store1\", 100),\n",
    "(\"2023-01-02\", \"Store1\", 150),\n",
    "(\"2023-01-03\", \"Store1\", 200),\n",
    "(\"2023-01-04\", \"Store1\", 250),\n",
    "(\"2023-01-05\", \"Store1\", 300),\n",
    "(\"2023-01-01\", \"Store2\", 50),\n",
    "(\"2023-01-02\", \"Store2\", 60),\n",
    "(\"2023-01-03\", \"Store2\", 80),\n",
    "(\"2023-01-04\", \"Store2\", 90),\n",
    "(\"2023-01-05\", \"Store2\", 120)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Date\", \"Store\", \"Sales\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"Date\", F.to_date(F.col(\"Date\")))\n",
    "\n",
    "windows = Window.orderBy(F.col(\"Date\")).partitionBy(F.col(\"Store\"))\n",
    "\n",
    "df = df.withColumn(\"lag_col\", F.lag(F.col(\"Sales\")).over(windows))\n",
    "df = df.withColumn(\"lead_col\", F.lead(F.col(\"Sales\")).over(windows))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65dd0a5-a695-4b49-b41e-2e17d524d6c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the frequency of unique values in the entire dataframe?"
    }
   },
   "outputs": [],
   "source": [
    "# How to get the frequency of unique values in the entire dataframe?\n",
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(1, 2, 3),\n",
    "(4, 5, 6),\n",
    "(2, 3, 4)]\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "df_col1 = df.select(\"Column1\")\n",
    "df_col2 = df.select(\"Column2\")\n",
    "df_col3 = df.select(\"Column3\")\n",
    "\n",
    "df = df_col1.unionAll(df_col2).unionAll(df_col3)\n",
    "\n",
    "df = df.groupBy(\"Column1\").count()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74fbff74-809a-458e-995b-97d8ab6f75a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = [\n",
    "    (1, 2, 3),\n",
    "    (2, 3, 4),\n",
    "    (1, 2, 3),\n",
    "    (4, 5, 6),\n",
    "    (2, 3, 4),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Step 1: Convert columns into an array for each row\n",
    "df_array = df.withColumn(\"values_array\", F.array(*[F.col(c) for c in df.columns]))\n",
    "print(\"DataFrame With Array Column:\")\n",
    "df_array.show()\n",
    "\n",
    "# Step 2: Explode the array to flatten values into rows\n",
    "df_exploded = df_array.select(F.explode(F.col(\"values_array\")).alias(\"value\"))\n",
    "print(\"Exploded DataFrame:\")\n",
    "df_exploded.show()\n",
    "\n",
    "# Step 3: Group by unique values and count occurrences\n",
    "df_frequency = df_exploded.groupBy(\"value\").count()\n",
    "print(\"Frequency of Unique Values Across Entire DataFrame:\")\n",
    "df_frequency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24122be7-6504-4487-8374-ce2ebb8d3324",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Replace both the diagonals of dataframe with 0"
    }
   },
   "outputs": [],
   "source": [
    "# How to replace both the diagonals of dataframe with 0\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(1, 2, 3, 4),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "column_list = df.columns\n",
    "max_row = df.count()\n",
    "max_col = len(column_list)\n",
    "\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(column_list[0], F.when(F.col(\"id\")==0, 0).otherwise(F.col(column_list[0])))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(column_list[-1], F.when(F.col(\"id\")==0, 0).otherwise(F.col(column_list[-1])))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(column_list[0], F.when(F.col(\"id\")==max_row-1, 0).otherwise(F.col(column_list[0])))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(column_list[-1], F.when(F.col(\"id\")==max_row-1, 0).otherwise(F.col(column_list[-1])))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a59db9f-0e51-41d4-822e-fd73dae76fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, lit, when\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create a 4x4 DataFrame\n",
    "data = [(1, 2, 3, 4),\n",
    "        (2, 3, 4, 5),\n",
    "        (1, 2, 3, 4),\n",
    "        (4, 5, 6, 7)]\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Step 1: Add row index column (id)\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Step 2: Compute the diagonal and anti-diagonal values\n",
    "column_list = df.columns[:-1]  # Exclude the 'id' column from columns to process\n",
    "n = len(column_list)           # Number of numeric columns\n",
    "\n",
    "print(column_list)\n",
    "\n",
    "# Replace diagonals\n",
    "for i, col_name in enumerate(column_list):\n",
    "    print(i, col_name)\n",
    "    main_diagonal_condition = F.col(\"id\") == i                        # Main diagonal (row == col)\n",
    "    anti_diagonal_condition = F.col(\"id\") + i == n - 1                # Anti-diagonal (row + col == n-1)\n",
    "    \n",
    "    # Apply conditional replacement for diagonals\n",
    "    df = df.withColumn(\n",
    "        col_name,\n",
    "        when(main_diagonal_condition | anti_diagonal_condition, lit(0)).otherwise(F.col(col_name))\n",
    "    )\n",
    "\n",
    "df = df.drop(\"id\")\n",
    "\n",
    "print(\"DataFrame with Diagonals Replaced with 0:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa5f26e2-93cd-4823-871b-3cd0731b0261",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reverse the rows of a dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# How to reverse the rows of a dataframe\n",
    "\n",
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(3, 4, 5, 6),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "df.show()\n",
    "\n",
    "df = df.orderBy(F.desc(\"id\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbb881b-f78c-4e6a-8d38-dc82e8c80503",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UnPivot the dataframe (converting columns into rows)"
    }
   },
   "outputs": [],
   "source": [
    "# How to UnPivot the dataframe (converting columns into rows) \n",
    "# UnPivot EU, US columns and create region, revenue Columns\n",
    "\n",
    "# Sample data\n",
    "data = [(2021, 2, 4500, 5500),\n",
    "(2021, 1, 4000, 5000),\n",
    "(2021, 3, 5000, 6000),\n",
    "(2021, 4, 6000, 7000)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"EU\", \"US\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.unpivot([\"year\", \"quarter\"], [\"EU\", \"US\"], \"region\", \"revenue\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1aac586-c176-4da9-a876-bba02253fdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Challenges",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
