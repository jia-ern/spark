{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744f23a3-45c4-4c5e-9b93-faa21c886724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame \n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "\n",
    "def map_to_spark_type(data_type):\n",
    "    if 'int' in data_type:\n",
    "        return IntegerType()\n",
    "    elif 'float' in data_type:\n",
    "        return DoubleType()\n",
    "    elif 'bool' in data_type:\n",
    "        return BooleanType()\n",
    "    elif 'datetime' in data_type:\n",
    "        return TimestampType()\n",
    "    else:\n",
    "        return StringType()\n",
    "\n",
    "\n",
    "def generate_schema_string(df, struct_type_name):\n",
    "\n",
    "    # if not isinstance(df, pyspark.sql.dataframe.DataFrame):\n",
    "    #     raise TypeError(\"The 'df' parameter must be a DataFrame or Dataset.\")\n",
    "\n",
    "    if df.columns:\n",
    "        columns = df.columns\n",
    "    else:\n",
    "        columns = [f\"col_{i + 1}\" for i in range(len(df.columns))]\n",
    "\n",
    "    print(\"before zip columns: \", columns)\n",
    "    print(\"before zip df.dtypes: \", df.dtypes)\n",
    "    print(\"before zip columns: \", columns)\n",
    "    print(\"before zip df.dtypes: \", df.dtypes)\n",
    "    print(\"zip: \", zip(columns, df.dtypes))\n",
    "\n",
    "    schema_string = f\"{struct_type_name} = StructType([\"\n",
    "    for col, dtype in zip(columns, df.dtypes):\n",
    "        spark_data_type = map_to_spark_type(dtype[1])\n",
    "        schema_string += f\"\\n    StructField('{col}', {spark_data_type}, True),\"\n",
    "\n",
    "    schema_string = schema_string.rstrip(',') + '\\n])'\n",
    "\n",
    "    return schema_string\n",
    "\n",
    "\n",
    "from sqlalchemy.types import (\n",
    "    BigInteger,\n",
    "    Boolean,\n",
    "    Date,\n",
    "    DateTime,\n",
    "    DECIMAL,\n",
    "    Float,\n",
    "    Integer,\n",
    "    Numeric,\n",
    "    SmallInteger,\n",
    "    String,\n",
    "    Unicode,\n",
    "    Text,\n",
    "    TIMESTAMP,\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    DecimalType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    ShortType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "__sqlalchemy_to_spark_type_mapping: dict = {\n",
    "    Integer: IntegerType,\n",
    "    SmallInteger: ShortType,\n",
    "    BigInteger: LongType,\n",
    "    Float: FloatType,\n",
    "    String: StringType,\n",
    "    Text: StringType,\n",
    "    Unicode: StringType,\n",
    "    Boolean: BooleanType,\n",
    "    Date: DateType,\n",
    "    DateTime: TimestampType,\n",
    "    Numeric: DecimalType,\n",
    "    DECIMAL: DecimalType,\n",
    "    TIMESTAMP: TimestampType,\n",
    "}\n",
    "\n",
    "\n",
    "def convert_to_spark_type(sqlalchemy_type):\n",
    "    return __sqlalchemy_to_spark_type_mapping[type(sqlalchemy_type)]\n",
    "\n",
    "def generate_model_schema(df, struct_type_name):\n",
    "    spark_fields = []\n",
    "\n",
    "    if df.columns:\n",
    "        columns = df.columns\n",
    "    else:\n",
    "        columns = [f\"col_{i + 1}\" for i in range(len(df.columns))]\n",
    "\n",
    "    for col, dtype in zip(columns, df.dtypes):\n",
    "        spark_data_type = convert_to_spark_type(dtype[1])\n",
    "        spark_fields.append(StructField(col, spark_data_type))\n",
    "    return StructType(spark_fields)         \n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ASG\").getOrCreate()\n",
    "df = spark.createDataFrame([(1, 'John', True, 100.5, '2022-01-01'),\n",
    "                            (2, 'Jane', False, 200.0, '2022-02-01')],\n",
    "                           ['id', 'name', 'status', 'amount', 'timestamp'])\n",
    "\n",
    "generated_schema_string = generate_schema_string(df , 'MyCustomSchema')\n",
    "\n",
    "print('Generated Schema String:')\n",
    "print(generated_schema_string)\n",
    "\n",
    "# MyCustomSchema = StructType([\n",
    "#     StructField('id', IntegerType(), True),\n",
    "#     StructField('name', StringType(), True),\n",
    "#     StructField('status', BooleanType(), True),\n",
    "#     StructField('amount', StringType(), True),\n",
    "#     StructField('timestamp', StringType(), True)\n",
    "# ])\n",
    "\n",
    "df_no_column_names = spark.createDataFrame([(1, 'John', True, 100.5, '2022-01-01'),\n",
    "                                            (2, 'Jane', False, 200.0, '2022-02-01')])\n",
    "\n",
    "generated_schema_string = generate_schema_string(df_no_column_names , 'MyCustomShema')\n",
    "print('Generated Schema without column name:')\n",
    "print(generated_schema_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e35d4a50-6379-4d83-8729-68f6df8b0e72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert alembic to Struct"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Type\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    DecimalType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    ShortType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "from sqlalchemy import Column, inspect\n",
    "from sqlalchemy.sql.type_api import TypeEngine\n",
    "from sqlalchemy.types import (\n",
    "    BigInteger,\n",
    "    Boolean,\n",
    "    Date,\n",
    "    DateTime,\n",
    "    DECIMAL,\n",
    "    Float,\n",
    "    Integer,\n",
    "    Numeric,\n",
    "    SmallInteger,\n",
    "    String,\n",
    "    Unicode,\n",
    "    Text,\n",
    "    TIMESTAMP,\n",
    ")\n",
    "\n",
    "from bh_shared.exceptions import DataTypeMappingException\n",
    "from bh_shared.models_types import (\n",
    "    ANY_SCHEMA,\n",
    "    HistoricalColumn,\n",
    "    OutputSchema,\n",
    "    RecordIntegrationStatus,\n",
    "    StagingSchema,\n",
    ")\n",
    "\n",
    "__sqlalchemy_to_spark_type_mapping: dict = {\n",
    "    Integer: IntegerType,\n",
    "    SmallInteger: ShortType,\n",
    "    BigInteger: LongType,\n",
    "    Float: FloatType,\n",
    "    String: StringType,\n",
    "    Text: StringType,\n",
    "    Unicode: StringType,\n",
    "    Boolean: BooleanType,\n",
    "    Date: DateType,\n",
    "    DateTime: TimestampType,\n",
    "    Numeric: DecimalType,\n",
    "    DECIMAL: DecimalType,\n",
    "    TIMESTAMP: TimestampType,\n",
    "}\n",
    "\n",
    "\n",
    "def convert_to_spark_type(sqlalchemy_type: TypeEngine):\n",
    "    return __sqlalchemy_to_spark_type_mapping[type(sqlalchemy_type)]\n",
    "\n",
    "\n",
    "def model_schema(model: Type[StagingSchema]) -> StructType:\n",
    "    columns = inspect(model).columns\n",
    "    spark_fields = []\n",
    "\n",
    "    for column in columns:\n",
    "        spark_data_type = convert_to_spark_type(column.type)\n",
    "\n",
    "        try:\n",
    "            if spark_data_type is DecimalType:\n",
    "                spark_type = spark_data_type(\n",
    "                    precision=getattr(column.type, \"precision\"),\n",
    "                    scale=getattr(column.type, \"scale\"),\n",
    "                )\n",
    "            else:\n",
    "                spark_type = spark_data_type()\n",
    "        except KeyError:\n",
    "            raise DataTypeMappingException(\n",
    "                f\"Do not know how to translate alembic's {str(column.type)} to Spark's data type.\"\n",
    "            )\n",
    "\n",
    "        spark_fields.append(StructField(column.name, spark_type, column.nullable))\n",
    "\n",
    "    return StructType(spark_fields)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "schema_autogenerator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
