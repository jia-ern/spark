{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daca48af-9792-4850-ae9b-4ad7c1c1b096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pydantic[email]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd7d579-46d7-44f8-a748-2b69fd76954f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark Schema Comparison UDF"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def compare_schemas(df_schema, delta_schema):\n",
    "\n",
    "    df_fields = {field.name: field.dataType.simpleString() for field in df_schema.fields}\n",
    "    delta_fields = {field.name: field.dataType.simpleString() for field in delta_schema.fields}\n",
    "\n",
    "    all_columns = set(df_fields.keys()).union(delta_fields.keys())\n",
    "    result = []\n",
    "\n",
    "    for col in all_columns:\n",
    "        src_type = df_fields.get(col, \"Missing\")\n",
    "        dest_type = delta_fields.get(col, \"Missing\")\n",
    "        \n",
    "        if src_type == dest_type:\n",
    "            status = \"Match\"\n",
    "        elif src_type == \"Missing\":\n",
    "            status = \"Missing in Source\"\n",
    "        elif dest_type == \"Missing\":\n",
    "            status = \"Missing in Destination\"\n",
    "        else:\n",
    "            status = \"Mismatch\"\n",
    " \n",
    "        result.append((col, src_type, dest_type, status))\n",
    " \n",
    "    return result\n",
    "\n",
    "# Register the UDF\n",
    "compare_schemas_udf = F.udf(lambda src, dest: compare_schemas(src, dest), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d71fb38-1ca9-4b22-a8dc-70e702709801",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Implementation"
    }
   },
   "outputs": [],
   "source": [
    "# Example Delta table schema and incoming DataFrame schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "delta_table_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "incoming_df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Compare schemas\n",
    "comparison_result = compare_schemas(incoming_df_schema, delta_table_schema)\n",
    "\n",
    "# Create a DataFrame from the result\n",
    "comparison_df = spark.createDataFrame(comparison_result, [\"Column Name\", \"Source Data Type\", \"Destination Data Type\", \"Status\"])\n",
    "\n",
    "# Show the comparison result\n",
    "comparison_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec1bca6-759d-4c4d-ba57-e1aebdc5f893",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optional - Compare schema using Pydantic"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, EmailStr\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: EmailStr\n",
    "\n",
    "user1 = User(name=\"Alice\", age=\"29\", email=\"alice@example.com\")\n",
    "print(\"User 1: \", user1)\n",
    "\n",
    "user2 = User(name=\"Bob\", age=\"not_a_number\", email=\"bob[at]mail\")\n",
    "print(user2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f9fcc5-7774-4710-9bc5-2420d5891fcf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pydantic"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "class Product(BaseModel):\n",
    "    product_id: str = Field(\n",
    "        description = \"This is the id for each product. Must be unique\"\n",
    "    )\n",
    "    product_name: str\n",
    "    price: float\n",
    "    currency: Literal[\"USD\", \"EUR\", \"GBP\"]\n",
    "    in_stock: bool\n",
    "    class Config:\n",
    "        anystr_lower = True  # Converts currency to lowercase\n",
    "\n",
    "product = Product(product_id=\"ABC123\", product_name=\"Apple\", price=12.00, currency=\"USD\", in_stock=True)\n",
    "print(\"Product: \", product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd44e53-bd56-48e7-b784-87de43915024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Union, Optional\n",
    "\n",
    "class MySecondModel(BaseModel):\n",
    "    first_name: str\n",
    "    middle_name: Union[str, None] # This means the parameter doesn't have to be sent\n",
    "    title: Optional[str] # this means the parameter should be sent, but can be None\n",
    "    last_name: str\n",
    "\n",
    "model1 = MySecondModel(first_name=\"Wong\", title=\"Ms\", last_name=\"Juliane\")\n",
    "print(model1)\n",
    "model2 = MySecondModel(first_name=\"Wong\", middle_name=\"\", title=\"Ms\", last_name=\"Juliane\")\n",
    "print(model2)\n",
    "model3 = MySecondModel(first_name=\"Wong\", middle_name=\"\", title=None, last_name=\"Juliane\")\n",
    "print(model3)\n",
    "model4 = MySecondModel(first_name=\"Wong\", middle_name=\"\", last_name=\"Juliane\")\n",
    "print(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39210a18-5c6d-4431-a378-4eef4750903d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pydantic - Deafult Values"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class DefaultsModel(BaseModel):\n",
    "    first_name: str = \"Jane\"\n",
    "    middle_names: list = []  # Problem: This list is shared across all instances for pydantic < 1.10 version\n",
    "    # middle_names: list = Field(default_factory=list)  # Creates a new list for each instance\n",
    "    last_name: str = \"Doe\"\n",
    "\n",
    "# Create multiple instances of the model\n",
    "model1 = DefaultsModel()\n",
    "model2 = DefaultsModel()\n",
    "\n",
    "# Modify `middle_names` in `model1`\n",
    "model1.middle_names.append(\"Marie\")\n",
    "\n",
    "# Check the contents of `middle_names` in both instances\n",
    "print(model1.middle_names)  # Output: ['Marie']\n",
    "print(model2.middle_names)  # Output: ['Marie']  <-- The same list is shared!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889e7f0a-3f9a-4c60-98da-5f291709a75f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6714b5c1-87a6-46b6-844d-7937e30353b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, model_validator, ValidationError\n",
    "from typing import Union, Any\n",
    "\n",
    "class AllOptionalAfterModel(BaseModel):\n",
    "    param1: Union[str, None] = None\n",
    "    param2: Union[str, None] = None\n",
    "    param3: Union[str, None] = None\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def there_must_be_one(self):\n",
    "        if not (self.param1 or self.param2 or self.param3):\n",
    "            raise ValidationError(\"One parameter must be specified\")\n",
    "        return self\n",
    "\n",
    "class AllOptionalBeforeModel(BaseModel):\n",
    "    param1: Union[str, None] = None\n",
    "    param2: Union[str, None] = None\n",
    "    param3: Union[str, None] = None\n",
    "    \n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def there_must_be_one(cls, data: Any):\n",
    "        if not (data[\"param1\"] or data[\"param2\"] or data[\"param3\"]):\n",
    "            raise ValidationError(\"One parameter must be specified\")\n",
    "        return data\n",
    "    \n",
    "model1 = AllOptionalAfterModel(param1=\"value1\")\n",
    "print(model1)\n",
    "model2 = AllOptionalAfterModel() \n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8829219-fa42-4396-aeb5-1a85f5448302",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Alias"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import AliasGenerator, BaseModel, ConfigDict\n",
    "\n",
    "class Tree(BaseModel):\n",
    "    model_config = ConfigDict(\n",
    "        alias_generator=AliasGenerator(\n",
    "            validation_alias=lambda field_name: field_name.upper(),\n",
    "            serialization_alias=lambda field_name: field_name.title(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    age: int\n",
    "    height: float\n",
    "    kind: str\n",
    "\n",
    "\n",
    "t = Tree.model_validate({'AGE': 12, 'HEIGHT': 1.2, 'KIND': 'oak'})\n",
    "print(t.model_dump(by_alias=True))\n",
    "\n",
    "t2 = Tree.model_validate({'age': 12, 'HEIGHT': 1.2, 'KIND': 'oak'})\n",
    "print(t2.model_dump(by_alias=True))\n",
    "#> {'Age': 12, 'Height': 1.2, 'Kind': 'oak'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ccee41d-cdaf-490a-a67a-849faa7f3bce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pydantic with non-flat (json) - AliasPath"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ConfigDict, AliasGenerator, AliasPath\n",
    "\n",
    "aliases = {\n",
    "    \"first_name\": AliasPath(\"name\", \"first_name\"),\n",
    "    \"last_name\": AliasPath(\"name\",  \"last_name\")\n",
    "}\n",
    "\n",
    "\n",
    "class FirstNameChoices(BaseModel):\n",
    "    model_config = ConfigDict(\n",
    "        alias_generator=AliasGenerator(\n",
    "            validation_alias=lambda field_name: aliases.get(field_name, None)\n",
    "        )\n",
    "    )\n",
    "    title: str\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "\n",
    "obj = FirstNameChoices(**{\"name\":{\"first_name\": \"marc\", \"last_name\": \"Nealer\"},\"title\":\"Master Of All\"})\n",
    "print(obj)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Schema Comparison UDF and Pydantic",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
