{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4b6afe2-76fc-4f07-80a7-0dbe38a1ae10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Type 0 (Fixed Dimension)\n",
    "- Description: \n",
    "  - No changes \n",
    "  - primary uses for static data are when the data does not change over time, for example; states, zip codes, county codes, SSN, date of birth etc.\n",
    "- Implementation: \n",
    "  - The records located in these tables are unalterable and no modification can be made to the record.\n",
    "\n",
    "\n",
    "# Type 1 (Overwrite)\n",
    "- Description: \n",
    "  - overwriting is used where the new value simply replaces the old value that was already stored. \n",
    "  - does not save any previous data; that is, it cannot illustrate changes over time.\n",
    "- Implementation: \n",
    "  - When an update happens, then the new value replaces the previous value in the database without any interference. \n",
    "  - applied in situations where the historical data is irrelevant to the task, eg: changing a customer’s current address.\n",
    "\n",
    "\n",
    "# Type 2 (Add New Row)\n",
    "- Description: \n",
    "  - a new record is added every time there is a change but the history is retained. \n",
    "  - when a new record is added, a new surrogate also gets created; \n",
    "  - physical relationships are preserved through the use of natural keys.\n",
    "- Implementation: \n",
    "  - by **adding is_current, start_date, end_date**\n",
    "  - enables the tracking of changes over time and is mostly applied to attributes such as product bundles.\n",
    "\n",
    "\n",
    "**Type 3 (Add New Attribute)**\n",
    "- Description: \n",
    "  - record changes by adding an attribute that will hold the prior value of an attribute. \n",
    "  - enables tracking of a few changes only; \n",
    "  - suitable where only the last change needs to be retained.\n",
    "- Implementation: \n",
    "  - previous value: (previous_address)\n",
    "  - current value: (current_address)\n",
    "  - suitable when charting change that is anticipated to occur periodically for example a change of the warehouse’s physical address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b154aaa6-4346-4f7e-aeda-d846875c195d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'workspace'\n",
    "schema = 'default'\n",
    "volume_name = 'spark_vol1'\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e008a65a-8cc2-4488-b26c-c0c269712d20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SCD Class"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType, StringType\n",
    "\n",
    "\n",
    "class SCDHandler:\n",
    "    def __init__(self, spark, delta_table_path, primary_keys, audit_columns=True):\n",
    "        \"\"\"\n",
    "        Initializes the SCDHandler class.\n",
    "\n",
    "        :param spark: Spark session object.\n",
    "        :param delta_table_path: Path to the Delta table.\n",
    "        :param primary_keys: List of primary keys for identifying records.\n",
    "        :param audit_columns: Whether to include audit columns (created_at, updated_at, etc.).\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.delta_table_path = delta_table_path\n",
    "        self.primary_keys = primary_keys\n",
    "        self.audit_columns = audit_columns\n",
    "\n",
    "    def _add_audit_columns(self, df, is_insert=True):\n",
    "        \"\"\"\n",
    "        Adds audit columns (created_at, updated_at) to a DataFrame.\n",
    "\n",
    "        :param df: Input DataFrame.\n",
    "        :param is_insert: If True, add both created_at and updated_at columns. For updates, only updated_at is added.\n",
    "        :return: DataFrame with audit columns.\n",
    "        \"\"\"\n",
    "        if is_insert:\n",
    "            return (\n",
    "                df.withColumn(\"created_at\", F.current_timestamp())\n",
    "                  .withColumn(\"updated_at\", F.current_timestamp())\n",
    "            )\n",
    "        else:\n",
    "            return df.withColumn(\"updated_at\", F.current_timestamp())\n",
    "\n",
    "    def _construct_merge_condition(self):\n",
    "        \"\"\"\n",
    "        Constructs the merge condition string based on primary keys.\n",
    "\n",
    "        :return: A string representing the merge condition.\n",
    "        \"\"\"\n",
    "        return \" AND \".join([f\"target.{key} = source.{key}\" for key in self.primary_keys])\n",
    "\n",
    "    def _initialize_delta_table(self, source_df, scd_type, effective_date_col, end_date_col, is_current_col, prev_value_columns):\n",
    "        \"\"\"\n",
    "        Initializes the Delta table if it does not exist.\n",
    "\n",
    "        :param source_df: Source DataFrame.\n",
    "        :param scd_type: Type of SCD (1, 2, or 3).\n",
    "        :param effective_date_col: Effective date column for SCD Type 2.\n",
    "        :param end_date_col: End date column for SCD Type 2.\n",
    "        :param is_current_col: Column indicating current record status for SCD Type 2/3.\n",
    "        :param prev_value_columns: List of previous value columns for SCD Type 3.\n",
    "        \"\"\"\n",
    "        if scd_type == 2:\n",
    "            source_df = (\n",
    "                source_df.withColumn(effective_date_col, F.current_timestamp())\n",
    "                         .withColumn(end_date_col, F.lit(None).cast(TimestampType()))\n",
    "                         .withColumn(is_current_col, F.lit(True))\n",
    "            )\n",
    "        elif scd_type == 3:\n",
    "            for col in prev_value_columns:\n",
    "                source_df = source_df.withColumn(f\"prev_{col}\", F.lit(None).cast(StringType()))\n",
    "        \n",
    "        if self.audit_columns:\n",
    "            source_df = self._add_audit_columns(source_df)\n",
    "\n",
    "        source_df.write.format(\"delta\").mode(\"overwrite\").save(self.delta_table_path)\n",
    "        print(f\"Delta table created at {self.delta_table_path}.\")\n",
    "\n",
    "    def _scd_type_1(self, delta_table, source_df, update_columns):\n",
    "        \"\"\"\n",
    "        Handles SCD Type 1 (overwrite on match).\n",
    "\n",
    "        :param delta_table: DeltaTable object.\n",
    "        :param source_df: Source DataFrame.\n",
    "        :param update_columns: Columns to update.\n",
    "        \"\"\"\n",
    "        merge_condition = self._construct_merge_condition()\n",
    "\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            source_df.alias(\"source\"), merge_condition\n",
    "        ).whenMatchedUpdate(\n",
    "            set={col: F.col(f\"source.{col}\") for col in update_columns}\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: F.col(col) for col in source_df.columns}\n",
    "        ).execute()\n",
    "\n",
    "    def _scd_type_2(self, delta_table, source_df, update_columns, effective_date_col, end_date_col, is_current_col):\n",
    "        \"\"\"\n",
    "        Handles SCD Type 2 (keep history).\n",
    "\n",
    "        :param delta_table: DeltaTable object.\n",
    "        :param source_df: Source DataFrame.\n",
    "        :param update_columns: Columns to update.\n",
    "        :param effective_date_col: Effective date column.\n",
    "        :param end_date_col: End date column.\n",
    "        :param is_current_col: Column indicating current record status.\n",
    "        \"\"\"\n",
    "        merge_condition = self._construct_merge_condition()\n",
    "\n",
    "        # source_df = self._add_audit_columns(source_df)\n",
    "\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            source_df.alias(\"source\"), merge_condition\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=f\"target.{is_current_col} = True AND (\" +\n",
    "                      \" OR \".join([f\"target.{col} != source.{col}\" for col in update_columns]) + \")\",\n",
    "            set={\n",
    "                **{col: F.col(f\"source.{col}\") for col in update_columns},\n",
    "                end_date_col: F.current_timestamp(),\n",
    "                is_current_col: F.lit(False),\n",
    "                # \"updated_at\": F.current_timestamp(),\n",
    "            }\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={\n",
    "                **{col: F.col(col) for col in source_df.columns},\n",
    "                effective_date_col: F.current_timestamp(),\n",
    "                end_date_col: F.lit(None).cast(TimestampType()),\n",
    "                is_current_col: F.lit(True),\n",
    "                # \"created_at\": F.current_timestamp(),\n",
    "            }\n",
    "        ).execute()\n",
    "\n",
    "    def _scd_type_3(self, delta_table, source_df, prev_value_columns):\n",
    "        \"\"\"\n",
    "        Handles SCD Type 3 (store previous values).\n",
    "\n",
    "        :param delta_table: DeltaTable object.\n",
    "        :param source_df: Source DataFrame.\n",
    "        :param prev_value_columns: List of previous value columns.\n",
    "        \"\"\"\n",
    "        merge_condition = self._construct_merge_condition()\n",
    "\n",
    "        for col in prev_value_columns:\n",
    "            if f\"prev_{col}\" not in source_df.columns:\n",
    "                source_df = source_df.withColumn(f\"prev_{col}\", F.lit(None).cast(StringType()))\n",
    "\n",
    "        # source_df = self._add_audit_columns(source_df)\n",
    "\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            source_df.alias(\"source\"), merge_condition\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\" OR \".join([f\"target.{col} != source.{col}\" for col in prev_value_columns]),\n",
    "            set={\n",
    "                **{col: F.col(f\"source.{col}\") for col in prev_value_columns},\n",
    "                **{f\"prev_{col}\": F.col(f\"target.{col}\") for col in prev_value_columns},\n",
    "                # \"updated_at\": F.current_timestamp(),\n",
    "            }\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={\n",
    "                **{col: F.col(col) for col in source_df.columns},\n",
    "                # is_current_col: F.lit(True),\n",
    "                # \"created_at\": F.current_timestamp()\n",
    "            }\n",
    "        ).execute()\n",
    "\n",
    "    def handle_scd(self, source_df, scd_type, update_columns=None, effective_date_col=\"effective_date\",\n",
    "                   end_date_col=\"end_date\", is_current_col=\"is_current\", prev_value_columns=None):\n",
    "        \"\"\"\n",
    "        Main handler for SCD operations.\n",
    "\n",
    "        :param source_df: Source DataFrame.\n",
    "        :param scd_type: SCD type (1, 2, or 3).\n",
    "        :param update_columns: List of columns to update.\n",
    "        :param effective_date_col: Effective date column for SCD Type 2.\n",
    "        :param end_date_col: End date column for SCD Type 2.\n",
    "        :param is_current_col: Column indicating current record status for SCD Type 2/3.\n",
    "        :param prev_value_columns: List of previous value columns for SCD Type 3.\n",
    "        \"\"\"\n",
    "        update_columns = update_columns or []\n",
    "        prev_value_columns = prev_value_columns or []\n",
    "\n",
    "        if not DeltaTable.isDeltaTable(self.spark, self.delta_table_path):\n",
    "            self._initialize_delta_table(source_df, scd_type, effective_date_col, end_date_col, is_current_col, prev_value_columns)\n",
    "            return\n",
    "\n",
    "        delta_table = DeltaTable.forPath(self.spark, self.delta_table_path)\n",
    "\n",
    "        if scd_type == 1:\n",
    "            self._scd_type_1(delta_table, source_df, update_columns)\n",
    "        elif scd_type == 2:\n",
    "            self._scd_type_2(delta_table, source_df, update_columns, effective_date_col, end_date_col, is_current_col)\n",
    "        elif scd_type == 3:\n",
    "            self._scd_type_3(delta_table, source_df, prev_value_columns)\n",
    "        else:\n",
    "            raise Exception(\"Invalid SCD type specified. Use 1, 2, or 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c2176862-2049-41b6-9709-68a3d930829c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data preparation"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Delta tables with initial data for testing\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, BooleanType\n",
    "\n",
    "# Use absolute paths for the Delta tables\n",
    "delta_table_path_scd1 = f'/Volumes/{catalog}/{schema}/{volume_name}/delta_table_scd1'\n",
    "delta_table_path_scd2 = f'/Volumes/{catalog}/{schema}/{volume_name}/delta_table_scd2'\n",
    "delta_table_path_scd3 = f'/Volumes/{catalog}/{schema}/{volume_name}/delta_table_scd3'\n",
    "\n",
    "# SCD Type 1 - Simple overwrite with no history\n",
    "schema_scd1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initial data for SCD Type 1\n",
    "data_initial_scd1 = [\n",
    "    (1, \"Alice\", 30, \"New York\"),\n",
    "    (2, \"Bob\", 35, \"Los Angeles\"),\n",
    "    (3, \"Charlie\", 40, \"Chicago\")\n",
    "]\n",
    "\n",
    "target_df_scd1 = spark.createDataFrame(data_initial_scd1, schema_scd1)\n",
    "target_df_scd1.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_scd1)\n",
    "\n",
    "# New source data for SCD Type 1\n",
    "data_source_scd1 = [\n",
    "    (1, \"Alice\", 32, \"Boston\"),  # Updated age and city\n",
    "    (4, \"David\", 28, \"San Francisco\")  # New record\n",
    "]\n",
    "source_df_scd1 = spark.createDataFrame(data_source_scd1, schema_scd1)\n",
    "\n",
    "# SCD Type 2 - Include effective_date, end_date, is_current columns\n",
    "schema_scd2 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"effective_date\", TimestampType(), True),\n",
    "    StructField(\"end_date\", TimestampType(), True),\n",
    "    StructField(\"is_current\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Initial data for SCD Type 2\n",
    "data_initial_scd2 = [\n",
    "    (1, \"Alice\", 30, \"New York\", None, None, True),\n",
    "    (2, \"Bob\", 35, \"Los Angeles\", None, None, True),\n",
    "    (3, \"Charlie\", 40, \"Chicago\", None, None, True)\n",
    "]\n",
    "\n",
    "target_df_scd2 = spark.createDataFrame(data_initial_scd2, schema_scd2)\n",
    "target_df_scd2.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_scd2)\n",
    "\n",
    "# New source data for SCD Type 2\n",
    "data_source_scd2 = [\n",
    "    (1, \"Alice\", 32, \"Boston\", None, None, True),  # Updated age and city\n",
    "    (4, \"David\", 28, \"San Francisco\", None, None, True)  # New record\n",
    "]\n",
    "source_df_scd2 = spark.createDataFrame(data_source_scd2, schema_scd2)\n",
    "\n",
    "# SCD Type 3 - Track previous city values\n",
    "schema_scd3 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"prev_city\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initial data for SCD Type 3\n",
    "data_initial_scd3 = [\n",
    "    (1, \"Alice\", 30, \"New York\", None),\n",
    "    (2, \"Bob\", 35, \"Los Angeles\", None),\n",
    "    (3, \"Charlie\", 40, \"Chicago\", None)\n",
    "]\n",
    "\n",
    "target_df_scd3 = spark.createDataFrame(data_initial_scd3, schema_scd3)\n",
    "target_df_scd3.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_scd3)\n",
    "\n",
    "# New source data for SCD Type 3\n",
    "data_source_scd3 = [\n",
    "    (1, \"Alice\", 32, \"Boston\", None),  # Updated city\n",
    "    (4, \"David\", 28, \"San Francisco\", None)  # New record\n",
    "]\n",
    "source_df_scd3 = spark.createDataFrame(data_source_scd3, schema_scd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ee083243-772e-4ff0-944e-a25fa42f373a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SCD Type 1"
    }
   },
   "outputs": [],
   "source": [
    "print(\"SCD Type 1: Initial Data\")\n",
    "scd1_df = spark.read.format('delta').load(f'/Volumes/{catalog}/{schema}/{volume_name}/delta_table_scd1')\n",
    "scd1_df.show(truncate=False)\n",
    "\n",
    "# Implementing SCD Type 1\n",
    "print(\"\\Implementing SCD Type 1...\")\n",
    "scd_handler1 = SCDHandler(\n",
    "    spark=spark,\n",
    "    delta_table_path=delta_table_path_scd1,\n",
    "    primary_keys=[\"id\"]\n",
    ")\n",
    "scd_handler1.handle_scd(source_df_scd1, scd_type=1, update_columns=[\"name\", \"age\", \"city\"])\n",
    "\n",
    "print(\"SCD Type 1: After incremental\")\n",
    "scd1_df = spark.read.format('delta').load(f'/Volumes/{catalog}/{schema}/{volume_name}/delta_table_scd1')\n",
    "scd1_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc97f52e-5804-4a4f-9566-c86783a87b6b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SCD Type 2 - To fix"
    }
   },
   "outputs": [],
   "source": [
    "print(\"SCD Type 2: Initial Data\")\n",
    "scd2_df = spark.read.format('delta').load(f'/Volumes/{catalog}/{schema}/{volume_name}/delta_table_scd2')\n",
    "scd2_df.show(truncate=False)\n",
    "\n",
    "# Implementing SCD Type 2\n",
    "print(\"Implementing SCD Type 2...\")\n",
    "scd_handler2 = SCDHandler(\n",
    "    spark=spark,\n",
    "    delta_table_path=delta_table_path_scd2,\n",
    "    primary_keys=[\"id\"]\n",
    ")\n",
    "scd_handler2.handle_scd(source_df_scd2, scd_type=2, update_columns=[\"name\", \"age\", \"city\"],         \n",
    "                        effective_date_col=\"effective_date\", end_date_col=\"end_date\", is_current_col=\"is_current\")\n",
    "\n",
    "print(\"SCD Type 2: After incremental\")\n",
    "scd2_df = spark.read.format('delta').load(f'/Volumes/{catalog}/{schema}/{volume_name}/delta_table_scd2')\n",
    "scd2_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b3d2e50-ef1a-47a8-a4e7-fa6284c2623a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SCD Type 3"
    }
   },
   "outputs": [],
   "source": [
    "print(\"SCD Type 3: Initial Data\")\n",
    "scd3_df = spark.read.format('delta').load(f'/Volumes/{catalog}/{schema}/{volume_name}/delta_table_scd3')\n",
    "scd3_df.show(truncate=False)\n",
    "\n",
    "# Implementing SCD Type 3\n",
    "print(\"Implementing SCD Type 3...\")\n",
    "scd_handler3 = SCDHandler(\n",
    "    spark=spark,\n",
    "    delta_table_path=delta_table_path_scd3,\n",
    "    primary_keys=[\"id\"]\n",
    ")\n",
    "scd_handler3.handle_scd(source_df_scd3, scd_type=3, update_columns=[\"name\", \"age\", \"city\"], prev_value_columns=[\"city\"])\n",
    "\n",
    "print(\"SCD Type 3: After incremental\")\n",
    "scd3_df = spark.read.format('delta').load(f'/Volumes/{catalog}/{schema}/{volume_name}/delta_table_scd3')\n",
    "scd3_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta_table_SCD",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
